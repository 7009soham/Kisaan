{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07975f16",
   "metadata": {},
   "source": [
    "# Run Kisaan ML without local GPU: Colab/Kaggle/Hugging Face options\n",
    "\n",
    "This notebook lets you execute the Kisaan Topic/Sub-topic classification problem even without a local GPU. Fill the config first, then choose one of: CPU baseline, Colab/Kaggle GPU, or Hugging Face Inference API.\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Project Variables (fill before running)\n",
    "\n",
    "# Configure your run here. For this Kisaan repo, defaults are set to text classification.\n",
    "CONFIG = {\n",
    "    \"problem_type\": \"classification\",  # classification|regression|seq2seq|vision\n",
    "    \"problem_statement_path\": \"docs/docs_problem_statement_Version2.md\",\n",
    "    # Local path or URL; for Colab/Kaggle, point to /content/drive/MyDrive/Kisaan/Datasets/... if using Drive\n",
    "    \"dataset_source\": \"Datasets/KCC_MarMay2025_combined.csv\",\n",
    "    # For this project, label is trained per-head. Here we drive a CPU baseline for topic only.\n",
    "    \"label_column\": \"topic\",\n",
    "    \"text_column\": \"QueryText\",\n",
    "    \"metrics\": [\"f1\", \"precision\", \"recall\", \"accuracy\"],\n",
    "    # Choose: cpu_baseline (sklearn), torch_cpu (slow), remote_api (HF Inference API)\n",
    "    \"model_choice\": \"cpu_baseline\",\n",
    "    # Remote inference (optional)\n",
    "    \"hf_model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"hf_token_env\": \"HF_TOKEN\",  # set this env var if using HF API\n",
    "    # Training/runtime knobs\n",
    "    \"batch_size\": 2048,\n",
    "    \"max_epochs\": 1,\n",
    "    \"internet_allowed\": True,\n",
    "}\n",
    "\n",
    "# Basic validation\n",
    "assert CONFIG[\"problem_type\"] in {\"classification\", \"regression\", \"seq2seq\", \"vision\"}\n",
    "assert isinstance(CONFIG[\"dataset_source\"], str) and len(CONFIG[\"dataset_source\"]) > 0\n",
    "print(\"Config loaded:\", json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41384bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Hardware and Runtime Detection\n",
    "import os, platform, sys, shutil, subprocess, psutil\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]} on {platform.platform()}\")\n",
    "print(f\"Pip: {shutil.which('pip')}\")\n",
    "print(f\"CWD: {os.getcwd()}\")\n",
    "\n",
    "# RAM and Disk\n",
    "try:\n",
    "    import psutil as _ps\n",
    "    mem = _ps.virtual_memory()\n",
    "    print(f\"RAM: {mem.total/1e9:.2f} GB total, {mem.available/1e9:.2f} GB available\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# GPU\n",
    "cuda_ok = torch.cuda.is_available()\n",
    "if cuda_ok:\n",
    "    print(\"CUDA available. Devices:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\" - {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader']).decode()\n",
    "        print(\"nvidia-smi:\\n\", out)\n",
    "    except Exception as e:\n",
    "        print(\"nvidia-smi not accessible:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Consider switching to Colab/Kaggle for training. This notebook includes CPU baselines and remote options.\")\n",
    "\n",
    "USE_GPU = bool(cuda_ok)\n",
    "print(\"USE_GPU:\", USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Runtime Helpers: Colab/Kaggle/Hugging Face Spaces Options\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = False\n",
    "IN_KAGGLE = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "IN_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "\n",
    "print({\"IN_COLAB\": IN_COLAB, \"IN_KAGGLE\": IN_KAGGLE})\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Colab detected. If needed, set Runtime -> Change runtime type -> GPU.\")\n",
    "    from google.colab import drive  # type: ignore\n",
    "    def mount_drive():\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"Drive mounted at /content/drive\")\n",
    "else:\n",
    "    def mount_drive():\n",
    "        print(\"Not in Colab; mount_drive() is a no-op.\")\n",
    "\n",
    "# (Optional) Helpers for creating a Hugging Face Space for inference\n",
    "SPACE_HELP = \"\"\"\n",
    "To deploy a small Gradio demo on Hugging Face Spaces:\n",
    "1) pip install gradio huggingface_hub\n",
    "2) huggingface-cli login  # use your HF token\n",
    "3) Create a repo: huggingface_hub.create_repo('your-username/kisaan-demo', repo_type='space', space_sdk='gradio')\n",
    "4) Push app.py and requirements.txt to that repo.\n",
    "\"\"\"\n",
    "print(\"Spaces helper available. See SPACE_HELP variable for steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4caf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) Dependency Installation (auto-detect or explicit)\n",
    "import os, sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install', '--quiet'] + pkgs\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    return subprocess.check_call(cmd)\n",
    "\n",
    "# Install if in Colab, or if user toggles via CONFIG[\"internet_allowed\"]\n",
    "if IN_COLAB and CONFIG.get(\"internet_allowed\", True):\n",
    "    pip_install([\n",
    "        'transformers>=4.41.0', 'datasets>=2.20.0', 'accelerate>=0.31.0', 'peft>=0.11.1', 'evaluate>=0.4.2',\n",
    "        'sentencepiece>=0.1.99', 'scikit-learn>=1.3', 'pandas>=2.0', 'numpy>=1.24', 'tqdm>=4.66', 'pyarrow>=14.0',\n",
    "        'matplotlib>=3.8', 'seaborn>=0.13', 'openpyxl>=3.1'\n",
    "    ])\n",
    "\n",
    "# Verify key imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print('pandas', pd.__version__, 'numpy', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Problem Statement Loader and Validator\n",
    "from pathlib import Path\n",
    "\n",
    "ps_path = Path(CONFIG[\"problem_statement_path\"]) \n",
    "if not ps_path.exists():\n",
    "    raise FileNotFoundError(f\"Problem statement missing: {ps_path}\")\n",
    "\n",
    "md = ps_path.read_text(encoding='utf-8')\n",
    "print(md.splitlines()[0:12])  # show first lines\n",
    "\n",
    "# Naive hints\n",
    "hints = {\n",
    "    'classification': any(k in md.lower() for k in ['classif', 'label', 'topic']),\n",
    "    'regression': 'regression' in md.lower(),\n",
    "    'seq2seq': any(k in md.lower() for k in ['translation','summariz','generate']),\n",
    "}\n",
    "print('Task hints:', hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6) Dataset Ingestion (local or remote)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = CONFIG[\"dataset_source\"]\n",
    "if data_path.startswith(\"http://\") or data_path.startswith(\"https://\"):\n",
    "    import urllib.request, tempfile\n",
    "    tmpf, _ = urllib.request.urlretrieve(data_path)\n",
    "    df = pd.read_csv(tmpf)\n",
    "else:\n",
    "    df = pd.read_csv(data_path, encoding='utf-8-sig')\n",
    "\n",
    "assert CONFIG[\"text_column\"] in df.columns, f\"Missing {CONFIG['text_column']} in dataset\"\n",
    "print(\"Rows:\", len(df), \"Columns:\", list(df.columns)[:20])\n",
    "print(df[[CONFIG[\"text_column\"]]].head())\n",
    "\n",
    "# If label exists, inspect balance (topic only here)\n",
    "if CONFIG[\"label_column\"] in df.columns:\n",
    "    vc = df[CONFIG[\"label_column\"]].fillna(\"Other\").astype(str).value_counts().head(20)\n",
    "    print(\"Top 20 labels:\\n\", vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7) Lightweight Baseline on CPU (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "if CONFIG[\"problem_type\"] == \"classification\":\n",
    "    texts = df[CONFIG[\"text_column\"]].fillna(\"\").astype(str)\n",
    "    labels = df.get(CONFIG[\"label_column\"])  # may be missing\n",
    "    if labels is None:\n",
    "        print(\"Label column not found; skipping baseline.\")\n",
    "    else:\n",
    "        y = labels.fillna(\"Other\").astype(str)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        pipe = Pipeline([\n",
    "            (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
    "            (\"clf\", LogisticRegression(max_iter=200, n_jobs=-1))\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        preds = pipe.predict(X_test)\n",
    "        print(classification_report(y_test, preds, zero_division=0))\n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_test, preds, average=\"micro\", zero_division=0),\n",
    "            \"macro_f1\": f1_score(y_test, preds, average=\"macro\", zero_division=0),\n",
    "            \"micro_p\": precision_score(y_test, preds, average=\"micro\", zero_division=0),\n",
    "            \"micro_r\": recall_score(y_test, preds, average=\"micro\", zero_division=0),\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "        }\n",
    "        print(\"Baseline metrics:\", metrics)\n",
    "        # Save baseline\n",
    "        import joblib\n",
    "        Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "        joblib.dump(pipe, \"artifacts/baseline_logreg_tfidf.joblib\")\n",
    "        json.dump(metrics, open(\"artifacts/baseline_metrics.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) Optional: PyTorch Training on CPU with Gradient Accumulation (slow)\n",
    "# For this project, we recommend Colab GPU for transformer fine-tuning. This cell is a stub.\n",
    "print(\"Skipping heavy CPU training; use Colab with the training cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9) Remote Inference via Hugging Face Inference API (generic example)\n",
    "import os, time, requests\n",
    "\n",
    "if CONFIG[\"model_choice\"] == \"remote_api\":\n",
    "    HF_TOKEN = os.environ.get(CONFIG[\"hf_token_env\"], None)\n",
    "    assert HF_TOKEN, f\"Set env var {CONFIG['hf_token_env']} to your HF token\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{CONFIG['hf_model_id']}\"\n",
    "\n",
    "    def hf_query(payload):\n",
    "        for attempt in range(5):\n",
    "            resp = requests.post(api_url, headers=headers, json=payload, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.json()\n",
    "            if resp.status_code in (503, 504):  # model loading / warmup\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "            raise RuntimeError(f\"HF API error {resp.status_code}: {resp.text}\")\n",
    "\n",
    "    sample_texts = df[CONFIG[\"text_column\"]].dropna().astype(str).head(3).tolist()\n",
    "    for t in sample_texts:\n",
    "        out = hf_query({\"inputs\": t})\n",
    "        print(t, \"\\n -> \", out, \"\\n\")\n",
    "else:\n",
    "    print(\"Remote API not selected; skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10) Evaluation and Metrics (for baseline)\n",
    "# Already printed in Section 7; this cell ensures metrics.json exists even if skipped.\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_path = Path(\"artifacts/baseline_metrics.json\")\n",
    "if metrics_path.exists():\n",
    "    print(metrics_path.read_text())\n",
    "else:\n",
    "    print(\"No baseline metrics available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11) Save Artifacts and Reproducibility\n",
    "import random, numpy as np, torch, subprocess, sys\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "with open(\"artifacts/config.json\",\"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Freeze environment for reproducibility (best-effort)\n",
    "try:\n",
    "    req_txt = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"]).decode()\n",
    "    Path(\"artifacts/requirements-locked.txt\").write_text(req_txt)\n",
    "except Exception as e:\n",
    "    print(\"Could not freeze environment:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12) Unit Tests Runner (pytest)\n",
    "# Minimal smoke tests inline\n",
    "try:\n",
    "    assert isinstance(CONFIG, dict) and CONFIG[\"problem_type\"] in {\"classification\",\"regression\",\"seq2seq\",\"vision\"}\n",
    "    assert len(df) > 0 and CONFIG[\"text_column\"] in df.columns\n",
    "    print(\"Smoke tests passed.\")\n",
    "except AssertionError as e:\n",
    "    raise AssertionError(f\"Smoke test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb989a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13) Troubleshooting and Auto-fixes\n",
    "# Common issues handler: adjust batch sizes, handle missing columns, unicode\n",
    "\n",
    "issues = []\n",
    "if CONFIG[\"text_column\"] not in df.columns:\n",
    "    issues.append(\"Missing text column\")\n",
    "if len(df) == 0:\n",
    "    issues.append(\"Empty dataset\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Detected issues:\", issues)\n",
    "    # Example auto-fix: do nothing here; guide the user\n",
    "    print(\"Please check dataset_source path and column names. For Colab, ensure Drive is mounted and paths are correct.\")\n",
    "else:\n",
    "    print(\"No common issues detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional: Kisaan Colab Training using repo scripts (GPU recommended)\n",
    "# Run this in Colab after mounting Drive and placing the repository under /content/drive/MyDrive/Kisaan\n",
    "\n",
    "if IN_COLAB:\n",
    "    mount_drive()\n",
    "    PROJECT_DIR = \"/content/drive/MyDrive/Kisaan\"\n",
    "    DATA_CSV = f\"{PROJECT_DIR}/Datasets/KCC_MarMay2025_combined.csv\"\n",
    "    OUT_TOPIC = f\"{PROJECT_DIR}/models/topic\"\n",
    "    OUT_SUB   = f\"{PROJECT_DIR}/models/subtopic\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(OUT_TOPIC, exist_ok=True)\n",
    "    os.makedirs(OUT_SUB, exist_ok=True)\n",
    "\n",
    "    # Install training deps (if needed)\n",
    "    if CONFIG.get(\"internet_allowed\", True):\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet',\n",
    "                               'transformers>=4.41.0','datasets>=2.20.0','accelerate>=0.31.0','peft>=0.11.1',\n",
    "                               'evaluate>=0.4.2','sentencepiece>=0.1.99','scikit-learn>=1.3','pandas>=2.0','numpy>=1.24','tqdm>=4.66','pyarrow>=14.0'])\n",
    "\n",
    "    # Run Topic training\n",
    "    !python {PROJECT_DIR}/src/train_topic_subtopic_peft.py --data_csv \"{DATA_CSV}\" --out_dir \"{OUT_TOPIC}\" --label_col topic --text_col QueryText --base_model xlm-roberta-base --epochs 4 --batch_size 16 --max_length 160 --lr 2e-5\n",
    "\n",
    "    # Run Sub-topic training\n",
    "    !python {PROJECT_DIR}/src/train_topic_subtopic_peft.py --data_csv \"{DATA_CSV}\" --out_dir \"{OUT_SUB}\" --label_col sub_topic --text_col QueryText --base_model xlm-roberta-base --epochs 4 --batch_size 16 --max_length 160 --lr 2e-5\n",
    "\n",
    "    # Inference (GPU/CPU auto)\n",
    "    OUT_SCORED = f\"{PROJECT_DIR}/Datasets/KCC_MarMay2025_scored.csv\"\n",
    "    !python {PROJECT_DIR}/src/predict_local.py --data_csv \"{DATA_CSV}\" --model_topic \"{OUT_TOPIC}\" --model_subtopic \"{OUT_SUB}\" --text_col QueryText --out_csv \"{OUT_SCORED}\" --device auto --batch_size 64\n",
    "\n",
    "    import pandas as pd\n",
    "    df_scored = pd.read_csv(OUT_SCORED, encoding='utf-8-sig')\n",
    "    print(df_scored.filter(regex='^(pred_|prob_topic::|prob_sub::|QueryText)').head())\n",
    "else:\n",
    "    print(\"Not in Colab; skipping GPU training cells.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
