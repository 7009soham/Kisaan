{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a55e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Setup and Environment Repair (run first)\n",
    "# - Pins NumPy/Pandas/PyArrow/Scikit-Learn to stable versions\n",
    "# - Silently uninstalls conflicting preinstalls and reinstalls pinned versions\n",
    "# - Creates project directories under /content/Kisaan\n",
    "# - Loads the training CSV safely with UTF-8-SIG and prints shape/columns\n",
    "# - Shows whether GPU is available (e.g., T4/L4)\n",
    "\n",
    "import sys, subprocess, warnings, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print('\\n=== Environment repair: pin scientific stack ===')\n",
    "\n",
    "# Helper to run pip quietly and suppress output\n",
    "\n",
    "def _pip(args):\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', *args],\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode != 0:\n",
    "        # Show truncated stderr on failure for quick diagnosis\n",
    "        tail = (result.stderr or '').strip().splitlines()[-10:]\n",
    "        raise RuntimeError('pip failed:\\n' + '\\n'.join(tail))\n",
    "\n",
    "# Uninstall and reinstall pinned versions to avoid binary/API mismatches\n",
    "PINNED = {\n",
    "    'numpy': '1.26.4',\n",
    "    'pandas': '2.2.2',\n",
    "    'pyarrow': '16.1.0',\n",
    "    'scikit-learn': '1.5.2',\n",
    "}\n",
    "try:\n",
    "    _pip(['uninstall', '-y', *PINNED.keys()])\n",
    "    _pip(['install', '--no-cache-dir', '--force-reinstall', '-q', *[f'{k}=={v}' for k, v in PINNED.items()]])\n",
    "    print('Pinned scientific stack installed: ' + ', '.join([f\"{k}=={v}\" for k, v in PINNED.items()]))\n",
    "except Exception as e:\n",
    "    print('Note: pip change encountered a non-fatal issue, continuing. Details:', e)\n",
    "\n",
    "# Verify imports and versions\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "import pyarrow as _pa\n",
    "import sklearn as _sk\n",
    "print(f\"numpy=={_np.__version__} | pandas=={_pd.__version__} | pyarrow=={_pa.__version__} | scikit-learn=={_sk.__version__}\")\n",
    "\n",
    "print('\\n=== Create project directories ===')\n",
    "PROJECT_DIR = Path('/content/Kisaan')\n",
    "RAW_DIR = PROJECT_DIR / 'Datasets' / 'raw'\n",
    "PROCESSED_DIR = PROJECT_DIR / 'Datasets' / 'processed'\n",
    "MODELS_DIR = PROJECT_DIR / 'models'\n",
    "for d in (PROJECT_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "print('Project dir:', PROJECT_DIR)\n",
    "print('Raw data dir:', RAW_DIR)\n",
    "print('Processed dir:', PROCESSED_DIR)\n",
    "\n",
    "print('\\n=== Runtime check (GPU/CPU) ===')\n",
    "_gpu_msg = 'No GPU detected. Tip: In Colab, enable GPU via Runtime → Change runtime type.'\n",
    "try:\n",
    "    import torch as _torch\n",
    "    if _torch.cuda.is_available():\n",
    "        try:\n",
    "            _gpu_name = _torch.cuda.get_device_name(0)\n",
    "        except Exception:\n",
    "            _gpu_name = 'CUDA device'\n",
    "        print('GPU available:', _gpu_name)\n",
    "    else:\n",
    "        print(_gpu_msg)\n",
    "except Exception:\n",
    "    # Fallback to nvidia-smi, if present\n",
    "    if shutil.which('nvidia-smi'):\n",
    "        out = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        line = (out.stdout or '').strip().splitlines()[0] if out.stdout else ''\n",
    "        print('GPU available:' if line else _gpu_msg, line)\n",
    "    else:\n",
    "        print(_gpu_msg)\n",
    "\n",
    "print('\\n=== Load CSV safely (UTF-8-SIG) ===')\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "DATASET_PATH = RAW_DIR / 'KCC_MarMay2025_train_ready_min2.csv'\n",
    "df_full = None\n",
    "if DATASET_PATH.exists():\n",
    "    try:\n",
    "        df_full = pd.read_csv(DATASET_PATH, encoding='utf-8-sig', low_memory=False, engine='python')\n",
    "        print(f'Loaded: {DATASET_PATH}')\n",
    "        print(f'Shape: {df_full.shape[0]:,} rows × {df_full.shape[1]:,} cols')\n",
    "        print('Columns:', list(df_full.columns))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Failed to read CSV at {DATASET_PATH}: {e}')\n",
    "else:\n",
    "    print('CSV not found at:', DATASET_PATH)\n",
    "    print('Please upload your file to this path, then re-run this cell.')\n",
    "\n",
    "print('\\nSetup complete. You can proceed to the next cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be35ee4",
   "metadata": {},
   "source": [
    "# Kisaan Training (Clean Colab Notebook)\n",
    "Use this notebook on Google Colab. It keeps dependencies minimal, avoids version pinning conflicts, and runs Topic/Sub-topic training end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4381c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project dir: \\content\\Kisaan\n",
      "Dataset path: \\content\\Kisaan\\Datasets\\KCC_MarMay2025_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Configure project paths (uses globals from cell 0)\n",
    "from pathlib import Path\n",
    "print('Project dir:', PROJECT_DIR)\n",
    "print('Dataset path:', DATASET_PATH)\n",
    "MODELS_DIR = MODELS_DIR  # already defined in cell 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0466885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IN_COLAB': False}\n",
      "Not running inside Colab; skipping mount.\n"
     ]
    }
   ],
   "source": [
    "# 2. (Optional) Mount Google Drive\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "print({'IN_COLAB': IN_COLAB})\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    DRIVE_PROJECT_DIR = Path('/content/drive/MyDrive/Kisaan')\n",
    "    print('Drive project dir:', DRIVE_PROJECT_DIR)\n",
    "else:\n",
    "    print('Not running inside Colab; skipping mount.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo already present.\n",
      "c:\\content\\Kisaan\n",
      "## main...origin/main\n",
      "?? Datasets/processed/\n",
      "## main...origin/main\n",
      "?? Datasets/processed/\n"
     ]
    }
   ],
   "source": [
    "# 3. Ensure repository code is present (robust clone/merge)\n",
    "import os, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = 'https://github.com/7009soham/Kisaan.git'\n",
    "TARGET_FILE = PROJECT_DIR / 'src' / 'train_topic_subtopic_peft.py'\n",
    "TMP_CLONE = PROJECT_DIR / '_repo_clone'\n",
    "\n",
    "needs_code = not TARGET_FILE.exists()\n",
    "if needs_code:\n",
    "    print('Project code missing; fetching from repo...')\n",
    "    if TMP_CLONE.exists():\n",
    "        shutil.rmtree(TMP_CLONE, ignore_errors=True)\n",
    "    subprocess.run(['git', 'clone', REPO_URL, str(TMP_CLONE)], check=True)\n",
    "\n",
    "    # Merge selected paths into PROJECT_DIR without disturbing your Datasets/models\n",
    "    def copy_into(src_dir: Path, dst_dir: Path, names: list[str]):\n",
    "        for name in names:\n",
    "            s = src_dir / name\n",
    "            if not s.exists():\n",
    "                continue\n",
    "            d = dst_dir / name\n",
    "            if s.is_dir():\n",
    "                shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "            else:\n",
    "                d.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "    copy_into(TMP_CLONE, PROJECT_DIR, [\n",
    "        'src', 'docs', 'notebooks', 'requirements-colab.txt', 'requirements-local.txt', 'README.md'\n",
    "    ])\n",
    "    shutil.rmtree(TMP_CLONE, ignore_errors=True)\n",
    "else:\n",
    "    print('Repository code already present.')\n",
    "\n",
    "# Show status; if this folder is a git repo, print status; otherwise just list tree\n",
    "%cd {PROJECT_DIR}\n",
    "if (PROJECT_DIR / '.git').exists():\n",
    "    !git status -sb\n",
    "else:\n",
    "    import itertools\n",
    "    print('Listing project tree (top-level):')\n",
    "    for p in itertools.islice(sorted(PROJECT_DIR.iterdir()), 0, 20):\n",
    "        print(' -', p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b85bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 4. Install ML dependencies (quiet)\n",
    "import sys, subprocess\n",
    "\n",
    "def _pipq(args):\n",
    "    r = subprocess.run([sys.executable, '-m', 'pip', *args],\n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if r.returncode != 0:\n",
    "        tail = (r.stderr or '').strip().splitlines()[-10:]\n",
    "        raise RuntimeError('pip failed:\\n' + '\\n'.join(tail))\n",
    "\n",
    "# Keep these versions compatible with our pinned scientific stack\n",
    "_pkgs = [\n",
    "    'transformers==4.44.2',\n",
    "    'datasets==2.20.0',\n",
    "    'accelerate==0.32.1',\n",
    "    'peft==0.12.0',\n",
    "    'sentencepiece'\n",
    "]\n",
    "_pipq(['install', '--quiet', '--no-cache-dir', *_pkgs])\n",
    "\n",
    "# Verify versions\n",
    "import transformers, datasets, accelerate, peft\n",
    "print('Installed:',\n",
    "      f\"transformers=={transformers.__version__}\",\n",
    "      f\"datasets=={datasets.__version__}\",\n",
    "      f\"accelerate=={accelerate.__version__}\",\n",
    "      f\"peft=={peft.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GPU tuning (optimized defaults; auto-detect A100)\n",
    "import os, torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print('Detected device:', GPU_NAME)\n",
    "\n",
    "# Prefer bf16 on Ampere+ (A100/H100/L4 etc.) and enable TF32 for speed\n",
    "try:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    import torch.backends.cuda as cuda_backend\n",
    "    import torch.backends.cudnn as cudnn_backend\n",
    "    cuda_backend.matmul.allow_tf32 = True\n",
    "    cudnn_backend.allow_tf32 = True\n",
    "    print('TF32 enabled for matmul and cuDNN')\n",
    "except Exception as _e:\n",
    "    print('TF32 setup skipped:', _e)\n",
    "\n",
    "# Dynamic training hyperparameters based on GPU\n",
    "USE_BF16 = torch.cuda.is_available() and any(x in GPU_NAME for x in ['A100','H100','L4','A10','A30','A40'])\n",
    "BATCH_SIZE = 32 if 'A100' in GPU_NAME else (24 if 'L4' in GPU_NAME else 16)\n",
    "MAX_LENGTH = 192 if 'A100' in GPU_NAME else 160\n",
    "\n",
    "print({'USE_BF16': USE_BF16, 'BATCH_SIZE': BATCH_SIZE, 'MAX_LENGTH': MAX_LENGTH})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset records: 42,086\n",
      "Columns: ['StateName', 'DistrictName', 'BlockName', 'Season', 'Sector', 'Category', 'Crop', 'QueryType', 'QueryText', 'KccAns', 'CreatedOn', 'year', 'month']\n"
     ]
    }
   ],
   "source": [
    "# 5. Verify dataset (robust finder with fallbacks and optional upload)\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# If df_full already loaded in cell 0, just summarize it\n",
    "if 'df_full' in globals() and isinstance(df_full, pd.DataFrame) and not df_full.empty:\n",
    "    print(f'Using df_full already loaded: {df_full.shape[0]:,} rows × {df_full.shape[1]:,} cols')\n",
    "    print('Columns:', list(df_full.columns))\n",
    "else:\n",
    "    found_path = None\n",
    "    candidates = [\n",
    "        DATASET_PATH,\n",
    "        PROJECT_DIR / 'Datasets' / 'KCC_MarMay2025_combined.csv',\n",
    "        PROJECT_DIR / 'Datasets' / 'KCC_MarMay2025.csv',\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            found_path = p\n",
    "            break\n",
    "    # Optional: search within project if not found in common locations (cheap glob)\n",
    "    if not found_path:\n",
    "        for p in (PROJECT_DIR / 'Datasets').glob('**/KCC_MarMay2025*.csv'):\n",
    "            found_path = p\n",
    "            break\n",
    "    if found_path:\n",
    "        DATASET_PATH = Path(found_path)  # update global for downstream cells\n",
    "        df_full = pd.read_csv(DATASET_PATH, encoding='utf-8-sig', low_memory=False, engine='python')\n",
    "        print(f'Loaded: {DATASET_PATH}')\n",
    "        print(f'Shape: {df_full.shape[0]:,} rows × {df_full.shape[1]:,} cols')\n",
    "        print('Columns:', list(df_full.columns))\n",
    "    else:\n",
    "        print('Dataset not found at expected paths.')\n",
    "        print('Looked for:')\n",
    "        for p in candidates:\n",
    "            print(' -', p)\n",
    "        try:\n",
    "            import google.colab  # type: ignore\n",
    "            from google.colab import files  # type: ignore\n",
    "            print('\\nYou can upload the CSV now; it will be saved as:', RAW_DIR / 'KCC_MarMay2025_train_ready_min2.csv')\n",
    "            uploaded = files.upload()\n",
    "            if uploaded:\n",
    "                name = next(iter(uploaded))\n",
    "                src = Path(name)\n",
    "                dst = RAW_DIR / 'KCC_MarMay2025_train_ready_min2.csv'\n",
    "                os.replace(src, dst)\n",
    "                DATASET_PATH = dst\n",
    "                df_full = pd.read_csv(DATASET_PATH, encoding='utf-8-sig', low_memory=False, engine='python')\n",
    "                print(f'Loaded after upload: {DATASET_PATH}')\n",
    "                print(f'Shape: {df_full.shape[0]:,} rows × {df_full.shape[1]:,} cols')\n",
    "                print('Columns:', list(df_full.columns))\n",
    "            else:\n",
    "                raise FileNotFoundError('Upload canceled. Please upload the CSV and re-run this cell.')\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError('CSV missing. In Colab, upload the file via this cell; otherwise place it at the RAW_DIR path and re-run.') from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote \\content\\Kisaan\\Datasets\\processed\\KCC_MarMay2025_topic_train.csv\n",
      "Wrote \\content\\Kisaan\\Datasets\\processed\\KCC_MarMay2025_sub_topic_train.csv\n",
      "Wrote \\content\\Kisaan\\Datasets\\processed\\KCC_MarMay2025_sub_topic_train.csv\n"
     ]
    }
   ],
   "source": [
    "# 6. Preprocess labels (ensure stratify works)\n",
    "import pandas as pd\n",
    "PROCESSED_DIR = PROJECT_DIR / 'Datasets' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse df_full if available; otherwise, read using DATASET_PATH (after cell 5 set it)\n",
    "if 'df_full' not in globals() or not isinstance(df_full, pd.DataFrame) or df_full.empty:\n",
    "    if not DATASET_PATH.exists():\n",
    "        raise FileNotFoundError(f'Dataset not found at {DATASET_PATH}. Please run the previous cell to locate/upload it.')\n",
    "    df_full = pd.read_csv(DATASET_PATH, encoding='utf-8-sig', low_memory=False, engine='python')\n",
    "\n",
    "\n",
    "def ensure_labels(df, col):\n",
    "    if col not in df.columns:\n",
    "        if col == 'topic' and 'QueryType' in df.columns:\n",
    "            df[col] = df['QueryType'].fillna('Other')\n",
    "        else:\n",
    "            df[col] = 'Other'\n",
    "    return df[col].fillna('Other').astype(str)\n",
    "\n",
    "\n",
    "def stabilize(series):\n",
    "    def first_label(val):\n",
    "        parts = [p.strip() for p in str(val).split(';') if p.strip()]\n",
    "        return parts[0] if parts else 'Other'\n",
    "    counts = series.apply(first_label).value_counts()\n",
    "    rare = set(counts[counts < 2].index)\n",
    "\n",
    "    def normalize(val):\n",
    "        parts = [p.strip() for p in str(val).split(';') if p.strip()]\n",
    "        if not parts:\n",
    "            return 'Other'\n",
    "        return 'Other' if parts[0] in rare else ';'.join(parts)\n",
    "\n",
    "    return series.apply(normalize)\n",
    "\n",
    "\n",
    "processed_paths = {}\n",
    "for col in ['topic', 'sub_topic']:\n",
    "    df_copy = df_full.copy()\n",
    "    df_copy[col] = stabilize(ensure_labels(df_copy, col))\n",
    "    out_path = PROCESSED_DIR / f'KCC_MarMay2025_{col}_train.csv'\n",
    "    df_copy.to_csv(out_path, index=False, encoding='utf-8-sig')\n",
    "    processed_paths[col] = out_path\n",
    "    print(f'Wrote {out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6A. Hotfix: ensure float32 labels in training script (for BCEWithLogits)\n",
    "from pathlib import Path\n",
    "script_path = PROJECT_DIR / 'src' / 'train_topic_subtopic_peft.py'\n",
    "if script_path.exists():\n",
    "    txt = script_path.read_text(encoding='utf-8')\n",
    "    changed = False\n",
    "    if 'astype(np.float32)' not in txt:\n",
    "        txt = txt.replace('mlb.fit_transform(y_train)', 'mlb.fit_transform(y_train).astype(np.float32)')\n",
    "        txt = txt.replace('mlb.transform(y_val)', 'mlb.transform(y_val).astype(np.float32)')\n",
    "        txt = txt.replace('mlb.transform(y_test)', 'mlb.transform(y_test).astype(np.float32)')\n",
    "        changed = True\n",
    "    if 'Dataset.from_dict({\"text\": X_train, \"labels\": list(y_train_bin)})' in txt:\n",
    "        txt = txt.replace('Dataset.from_dict({\"text\": X_train, \"labels\": list(y_train_bin)})',\n",
    "                          'Dataset.from_dict({\"text\": X_train, \"labels\": [row.tolist() for row in y_train_bin]})')\n",
    "        changed = True\n",
    "    if 'Dataset.from_dict({\"text\": X_val,   \"labels\": list(y_val_bin)})' in txt:\n",
    "        txt = txt.replace('Dataset.from_dict({\"text\": X_val,   \"labels\": list(y_val_bin)})',\n",
    "                          'Dataset.from_dict({\"text\": X_val,   \"labels\": [row.tolist() for row in y_val_bin]})')\n",
    "        changed = True\n",
    "    if 'Dataset.from_dict({\"text\": X_test,  \"labels\": list(y_test_bin)})' in txt:\n",
    "        txt = txt.replace('Dataset.from_dict({\"text\": X_test,  \"labels\": list(y_test_bin)})',\n",
    "                          'Dataset.from_dict({\"text\": X_test,  \"labels\": [row.tolist() for row in y_test_bin]})')\n",
    "        changed = True\n",
    "    if changed:\n",
    "        script_path.write_text(txt, encoding='utf-8')\n",
    "        print('Applied float32 labels hotfix to training script.')\n",
    "    else:\n",
    "        print('Training script already ensures float32 labels. No changes made.')\n",
    "else:\n",
    "    print('Training script not found at', script_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\n",
      "Map:   0%|          | 0/33668 [00:00<?, ? examples/s]\n",
      "Map:  18%|█▊        | 6000/33668 [00:00<00:00, 49915.45 examples/s]\n",
      "Map:  33%|███▎      | 11000/33668 [00:00<00:00, 34011.90 examples/s]\n",
      "Map:  48%|████▊     | 16000/33668 [00:00<00:00, 34536.39 examples/s]\n",
      "Map:  62%|██████▏   | 21000/33668 [00:00<00:00, 36748.25 examples/s]\n",
      "Map:  80%|████████  | 27000/33668 [00:00<00:00, 40041.30 examples/s]\n",
      "Map:  98%|█████████▊| 33000/33668 [00:00<00:00, 42449.65 examples/s]\n",
      "Map: 100%|██████████| 33668/33668 [00:00<00:00, 39895.26 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/4209 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 4209/4209 [00:00<00:00, 48542.46 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/4209 [00:00<?, ? examples/s]\n",
      "Map:  48%|████▊     | 2000/4209 [00:00<00:00, 16148.46 examples/s]\n",
      "Map: 100%|██████████| 4209/4209 [00:00<00:00, 25131.11 examples/s]\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/8420 [00:00<?, ?it/s]c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\content\\Kisaan\\src\\train_topic_subtopic_peft.py\", line 247, in <module>\n",
      "    main()\n",
      "  File \"c:\\content\\Kisaan\\src\\train_topic_subtopic_peft.py\", line 211, in main\n",
      "    trainer.train()\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 2279, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 3318, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 3363, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\peft\\peft_model.py\", line 1379, in forward\n",
      "    return self.base_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py\", line 188, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py\", line 1242, in forward\n",
      "    loss = loss_fct(logits, labels)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 850, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3593, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\n",
      "  0%|          | 0/8420 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 7. Train Topic head\n",
    "topic_csv = processed_paths['topic']\n",
    "topic_out = MODELS_DIR / 'topic'\n",
    "topic_out.mkdir(parents=True, exist_ok=True)\n",
    "!python src/train_topic_subtopic_peft.py --data_csv \"{topic_csv}\" --out_dir \"{topic_out}\" --label_col topic --text_col QueryText --base_model xlm-roberta-base --epochs 4 --batch_size {BATCH_SIZE} --max_length {MAX_LENGTH} --lr 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead45097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\n",
      "Map:   0%|          | 0/33668 [00:00<?, ? examples/s]\n",
      "Map:  18%|█▊        | 6000/33668 [00:00<00:00, 46840.97 examples/s]\n",
      "Map:  33%|███▎      | 11000/33668 [00:00<00:00, 33553.92 examples/s]\n",
      "Map:  50%|█████     | 17000/33668 [00:00<00:00, 38868.10 examples/s]\n",
      "Map:  68%|██████▊   | 23000/33668 [00:00<00:00, 41860.14 examples/s]\n",
      "Map:  86%|████████▌ | 29000/33668 [00:00<00:00, 42770.25 examples/s]\n",
      "Map: 100%|██████████| 33668/33668 [00:00<00:00, 41328.39 examples/s]\n",
      "Map: 100%|██████████| 33668/33668 [00:00<00:00, 40687.92 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/4209 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 4209/4209 [00:00<00:00, 43178.38 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/4209 [00:00<?, ? examples/s]\n",
      "Map:  48%|████▊     | 2000/4209 [00:00<00:00, 15391.46 examples/s]\n",
      "Map: 100%|██████████| 4209/4209 [00:00<00:00, 23739.84 examples/s]\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/8420 [00:00<?, ?it/s]c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\content\\Kisaan\\src\\train_topic_subtopic_peft.py\", line 247, in <module>\n",
      "    main()\n",
      "  File \"c:\\content\\Kisaan\\src\\train_topic_subtopic_peft.py\", line 211, in main\n",
      "    trainer.train()\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 2279, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 3318, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\trainer.py\", line 3363, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\peft\\peft_model.py\", line 1379, in forward\n",
      "    return self.base_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py\", line 188, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py\", line 1242, in forward\n",
      "    loss = loss_fct(logits, labels)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 850, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3593, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\n",
      "  0%|          | 0/8420 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 8. Train Sub-topic head\n",
    "sub_csv = processed_paths['sub_topic']\n",
    "sub_out = MODELS_DIR / 'subtopic'\n",
    "sub_out.mkdir(parents=True, exist_ok=True)\n",
    "!python src/train_topic_subtopic_peft.py --data_csv \"{sub_csv}\" --out_dir \"{sub_out}\" --label_col sub_topic --text_col QueryText --base_model xlm-roberta-base --epochs 4 --batch_size {BATCH_SIZE} --max_length {MAX_LENGTH} --lr 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b036e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored CSV: \\content\\Kisaan\\Datasets\\KCC_MarMay2025_scored.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\content\\Kisaan\\src\\predict_local.py\", line 94, in <module>\n",
      "    main()\n",
      "  File \"c:\\content\\Kisaan\\src\\predict_local.py\", line 67, in main\n",
      "    tok_t, mdl_t, labels_t, thr_t = load_head(Path(args.model_topic))\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\content\\Kisaan\\src\\predict_local.py\", line 25, in load_head\n",
      "    labels = json.loads((model_dir / \"labels.json\").read_text(encoding=\"utf-8\"))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 1058, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 1044, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '\\\\content\\\\Kisaan\\\\models\\\\topic\\\\labels.json'\n"
     ]
    }
   ],
   "source": [
    "# 9. Run inference on combined CSV\n",
    "scored_csv = PROJECT_DIR / 'Datasets' / 'KCC_MarMay2025_scored.csv'\n",
    "!python src/predict_local.py --data_csv \"{DATASET_PATH}\" --model_topic \"{topic_out}\" --model_subtopic \"{sub_out}\" --text_col QueryText --out_csv \"{scored_csv}\" --device auto --batch_size 64\n",
    "print('Scored CSV:', scored_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8216f95a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\content\\\\Kisaan\\\\Datasets\\\\KCC_MarMay2025_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 10. Preview predictions\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_scored = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscored_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8-sig\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_scored[[\u001b[33m'\u001b[39m\u001b[33mQueryText\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpred_topic\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpred_sub_topic\u001b[39m\u001b[33m'\u001b[39m]].head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\DeepLearningProjects\\Kisaan\\kisaanev\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '\\\\content\\\\Kisaan\\\\Datasets\\\\KCC_MarMay2025_scored.csv'"
     ]
    }
   ],
   "source": [
    "# 10. Preview predictions\n",
    "import pandas as pd\n",
    "df_scored = pd.read_csv(scored_csv, encoding='utf-8-sig')\n",
    "print(df_scored[['QueryText', 'pred_topic', 'pred_sub_topic']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75616a9",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If you need faster experiments, reduce `--epochs` to 1.\n",
    "- After training, download `models/topic`, `models/subtopic`, and the scored CSV for local inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kisaanev (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
